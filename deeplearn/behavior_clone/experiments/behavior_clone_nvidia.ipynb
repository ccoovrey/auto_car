{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavior Clone Experiment: NVIDIA\n",
    "We are using one lap of data, where driver tries to stay in the middle of the road. Now we are doing data processing and augmentation:\n",
    "* normalizing the data and mean centering\n",
    "* data augmentation by flipping the image and steering measurements\n",
    "* images are cropped\n",
    "* images are seen from multiple camera angles\n",
    "\n",
    "The model is based on nvidia model. This model has the following 5 convolutional layers:\n",
    "* convolutional layer 1: 24 5x5 filters with 2x2 stride\n",
    "* convolutional layer 2: 36 5x5 filters with 2x2 stride\n",
    "* convolutional layer 3: 48 5x5 filters with 2x2 stride\n",
    "* convolutional layer 4: 64 3x3 filters with single stride\n",
    "* convolutional layer 5: 64 3x3 filters with single stride with dropout\n",
    "* fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "lines = []\n",
    "with open('../../../../data/bc_one_lap/driving_log.csv') as csvfile:\n",
    "    rd = csv.reader(csvfile)\n",
    "    for line in rd:\n",
    "        lines.append(line)\n",
    "        \n",
    "        \n",
    "images = []\n",
    "measurements = []\n",
    "# update path \n",
    "for line in lines:\n",
    "    current_path = '../../../../data/bc_one_lap/IMG/' \n",
    "    img_center = cv2.imread(current_path + line[0].split('/')[-1])\n",
    "    img_left = cv2.imread(current_path + line[1].split('/')[-1])\n",
    "    img_right = cv2.imread(current_path + line[2].split('/')[-1])\n",
    "    images.extend([img_center, img_left, img_right, cv2.flip(img_center, 1), cv2.flip(img_left, 1),\n",
    "                   cv2.flip(img_right, 1)])\n",
    "    steer_center = float(line[3])\n",
    "    correction = 0.2 # parameter to tune\n",
    "    steer_left = steer_center + correction\n",
    "    steer_right = steer_center - correction\n",
    "    measurements.extend([steer_center, steer_left, steer_right, steer_center*-1.0, steer_left*-1.0,\n",
    "                         steer_right*-1.0])\n",
    "    \n",
    "# now convert to numpy arrays for keras\n",
    "X_train = np.array(images)\n",
    "y_train = np.array(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 160, 320, 3)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)        (None, 65, 320, 3)    0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 31, 158, 24)   1824        cropping2d_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 14, 77, 36)    21636       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 5, 37, 48)     43248       convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 3, 35, 64)     27712       convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 1, 33, 64)     36928       convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1, 33, 64)     0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 2112)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 250)           528250      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             251         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 659,849\n",
      "Trainable params: 659,849\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Convolution2D, Dropout, MaxPooling2D, Lambda, Cropping2D\n",
    "input_shape=(160,320,3)\n",
    "\n",
    "# most NN model with 2 layers and dropout\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "model.add(Convolution2D(24,5,5,subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(36,5,5,subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(48,5,5,subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(64,3,3,activation='relu'))\n",
    "model.add(Convolution2D(64,3,3,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7132 samples, validate on 1784 samples\n",
      "Epoch 1/3\n",
      "7132/7132 [==============================] - 61s - loss: 0.0260 - val_loss: 0.0135\n",
      "Epoch 2/3\n",
      "7132/7132 [==============================] - 61s - loss: 0.0186 - val_loss: 0.0165\n",
      "Epoch 3/3\n",
      "7132/7132 [==============================] - 66s - loss: 0.0155 - val_loss: 0.0236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9b83194a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(X_train, y_train, validation_split=0.2,\n",
    "          shuffle=True, nb_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Using this model epochs were 3 times faster than the last experiment, also using this model the autonomous car went around track one for one lap. There were a few times when the car came close to the side of the road, but\n",
    "the autonomous car corrrected itself. At times the autonomous car was eractic, so the next experiment is going to use more training data.\n",
    "\n",
    "This can be seen in the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"nvidia_onelap.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"nvidia_onelap.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
